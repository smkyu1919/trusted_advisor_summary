#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Trusted Advisor ì²´í¬ ê²°ê³¼ë¥¼ ë‹¨ì¼ ì‹œíŠ¸(All)ë¡œ ì¶œë ¥
- Category ìˆœ ì •ë ¬
- ê° ì²´í¬ í–‰ì— 'ë¦¬ì†ŒìŠ¤ ê°’' ì»¬ëŸ¼ ì¶”ê°€ (ì •ìƒ ë¸”ë¡ì€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ)
- 'ìµœì¢… ìƒíƒœ'ëŠ” ì •ìƒ/ì£¼ì˜/ê²½ê³ /ë¯¸í™•ì¸ ìš”ì•½
- ì œì™¸(isSuppressed=True)ëŠ” ì¹´ìš´íŠ¸ë§Œ ë°˜ì˜
- 'ì†ŒìŠ¤ ì¢…ë¥˜' ì»¬ëŸ¼ (í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì •)

rv5m.5 (2025-10-22)
- 2ìˆœìœ„(Name í¬í•¨ íƒ­): meta_headers ìˆœì„œëŒ€ë¡œ ìŠ¤ìº”í•˜ë©°, ì‚¬ëŒì¹œí™” ê°’ì´ë©´ ì¦‰ì‹œ ì±„íƒ (DB Instance Name ê°•ì œ ë°˜ì˜)
- 1ìˆœìœ„ ë³´ì™„: (ë©”íƒ€)resource ê°’ì´ íƒ€ì…í† í°(ì˜ˆ: db.t3.micro) / ë¦¬ì „ ë‹¨ë… / ë…¸ì´ì¦ˆë©´ ìŠ¤í‚µí•˜ì—¬ 2ìˆœìœ„ë¡œ ë–¨ì–´ì§€ê²Œ í•¨
- íƒ€ì…í† í° íŒì • ì¶”ê°€(INSTANCE_CLASS_RE) ë° _is_human_name()ì— ë°˜ì˜
- 6ìˆœìœ„(Service Limits/Quotas): Region | Service | Limit Amount | Current Usage | Status ë™ì˜ì–´ ë§¤í•‘ + í—¤ë” ìˆœì„œ ìš°ì„ 
- 8ìˆœìœ„: Recommended Resource Summary / Recommendation Summary ìµœí›„ ë³´ë£¨
- ë‚˜ë¨¸ì§€(ë‚œìˆ˜ í† í° ì°¨ë‹¨, ARN ì¶•ì•½, Strong/Instance/Check/ì „ì²´ì¡°í•©)ëŠ” ë³€ê²½ ì—†ìŒ
"""

import sys
import time
import re
from typing import Dict, List, Tuple, DefaultDict
from collections import defaultdict

import boto3
import botocore
import pandas as pd
from openpyxl.styles import Alignment
from openpyxl.utils import get_column_letter

VERSION = "rv5m.5-2025-10-22-007"
OUTPUT_XLSX = "trusted_advisor_checks.xlsx"

LANGUAGE = "en"              # en ë˜ëŠ” ja
SUPPORT_REGION = "us-east-1" # TA API ê³ ì •

# âš ï¸ ìš´ì˜ì—ì„œëŠ” í•˜ë“œì½”ë“œ ìê²©ì¦ëª… ì‚¬ìš© ì§€ì–‘(ì‚¬ìš©ì ì œê³µ ì½”ë“œ ìœ ì§€)
session = boto3.Session(
    aws_access_key_id=" ",
    aws_secret_access_key=" ",
    region_name=SUPPORT_REGION,
)

PRETTY_PILLAR: Dict[str, str] = {
    "cost_optimizing": "Cost Optimization",
    "cost_optimization": "Cost Optimization",
    "performance": "Performance",
    "security": "Security",
    "fault_tolerance": "Fault Tolerance",
    "operational_excellence": "Operational Excellence",
    "service_limits": "Service Limits",
    "service_quotas": "Service Limits",
}

STATUS_MAP_FINAL = {"error": "ê²½ê³ ", "warning": "ì£¼ì˜", "ok": "ì •ìƒ"}

def pretty_pillar_name(raw: str) -> str:
    if not raw:
        return "Unknown"
    raw_lc = raw.strip().lower()
    return PRETTY_PILLAR.get(raw_lc, raw_lc.replace("_", " ").title())

def backoff_sleep(i: int):
    time.sleep(min(2 ** i, 10))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íŒ¨í„´/ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ARN_RE = re.compile(r"^arn:(aws|aws-cn|aws-us-gov):([a-z0-9\-]+):([a-z0-9\-]*):(\d{12})?:(.+)$")
LAMBDA_FN_ARN_RE = re.compile(r"arn:[^:]+:lambda:[^:]+:\d{12}:function[:/][A-Za-z0-9-_]+(?::[A-Za-z0-9\-\$._+]+)?")

VOL_RE = re.compile(r"\bvol-[0-9a-fA-F]{8,}\b")
I_RE   = re.compile(r"\bi-[0-9a-fA-F]{8,}\b")
SG_RE  = re.compile(r"\bsg-[0-9a-fA-F]{8,}\b")
ENI_RE = re.compile(r"\beni-[0-9a-fA-F]{8,}\b")
SNAP_RE= re.compile(r"\bsnap-[0-9a-fA-F]{8,}\b")

TG_PATH_RE  = re.compile(r"\btargetgroup/[A-Za-z0-9-]+/[0-9a-fA-F]{6,}\b")
ALB_PATH_RE = re.compile(r"\bapp/[A-Za-z0-9-]+/[0-9a-fA-F]{6,}\b")
NLB_PATH_RE = re.compile(r"\bnet/[A-Za-z0-9-]+/[0-9a-fA-F]{6,}\b")

RDS_ARN_RE  = re.compile(r"arn:[^:]+:rds:[^:]+:\d{12}:(?:db|cluster):[A-Za-z0-9-]+")
KMS_ARN_RE  = re.compile(r"arn:[^:]+:kms:[^:]+:\d{12}:(?:key|alias)/[A-Za-z0-9/_\-]+")
IAM_ARN_RE  = re.compile(r"arn:[^:]+:iam::\d{12}:(?:role|user|group|policy)/[A-Za-z0-9+=,.@\-_/]+")

DDB_ARN_RE  = re.compile(r"arn:[^:]+:dynamodb:[^:]+:\d{12}:table/[A-Za-z0-9_.\-]+")
SNS_ARN_RE  = re.compile(r"arn:[^:]+:sns:[^:]+:\d{12}:[A-Za-z0-9_.\-]+")
SQS_ARN_RE  = re.compile(r"arn:[^:]+:sqs:[^:]+:\d{12}:[A-Za-z0-9_.\-]+")
ECR_ARN_RE  = re.compile(r"arn:[^:]+:ecr:[^:]+:\d{12}:repository/[A-Za-z0-9/_\-]+")
EKS_ARN_RE  = re.compile(r"arn:[^:]+:eks:[^:]+:\d{12}:cluster/[A-Za-z0-9._\-]+")
ECS_ARN_RE  = re.compile(r"arn:[^:]+:ecs:[^:]+:\d{12}:(?:service|cluster)/[A-Za-z0-9/_\-]+")
CFN_DISTR_ID_RE = re.compile(r"\bE[A-Z0-9]{12}\b")

ELASTICACHE_RE = re.compile(r"\b(arn:[^:]+:elasticache:[^:]+:\d{12}:.+|cache\.[a-z0-9\-]+|[A-Za-z0-9\-]*cache[A-Za-z0-9\-]*)\b")
REDSHIFT_ARN_RE= re.compile(r"arn:[^:]+:redshift:[^:]+:\d{12}:(?:cluster|dbname):[A-Za-z0-9\-]+")

OPENSEARCH_ARN_RE = re.compile(r"arn:[^:]+:(?:es|opensearch):[^:]+:\d{12}:domain/[A-Za-z0-9\-]+")
ROUTE53_HOSTEDZONE_RE = re.compile(r"/hostedzone/[A-Z0-9]+")
ROUTE53_ZONE_NAME_RE  = re.compile(r"^[A-Za-z0-9.-]+\.[A-Za-z]{2,}$")

BUCKETISH_RE = re.compile(r"^[a-z0-9][a-z0-9\.\-]{1,61}[a-z0-9]$")
HUMAN_NAME_RE = re.compile(r"^[A-Za-z0-9][A-Za-z0-9._\-\/]{2,200}$")
NOISE_TOKEN_RE = re.compile(r"^[A-Za-z0-9\-_]{24,}$")

# RDS DB ì¸ìŠ¤í„´ìŠ¤ ì´ë¦„/ì‹ë³„ì í˜•íƒœ í—ˆìš© (ì†Œë¬¸ì/ìˆ«ì/í•˜ì´í”ˆ, ìµœëŒ€ 63ì)
# RDS DB ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì: ì†Œë¬¸ì/ìˆ«ì/í•˜ì´í”ˆ, ìµœëŒ€ 63ì
RDS_IDENTIFIER_RE = re.compile(r"^[a-z][a-z0-9-]{0,62}$")
AZ_RE = re.compile(r"^(?:us|ap|eu|me|af|sa|ca)-[a-z]+-\d+[a-z]$", re.I)




REGION_RE = re.compile(
    r"^(?:us|ap|eu|me|af|sa|ca)-(?:north|south|east|west|northeast|southeast|central|southwest|northwest)\-\d+$|^global$",
    re.I,
)

# â–¼ ì¶”ê°€: EC2/RDS ë“± ì¸ìŠ¤í„´ìŠ¤ í´ë˜ìŠ¤(íƒ€ì… í† í°) íŒì •
INSTANCE_CLASS_RE = re.compile(
    r"^(?:db\.[a-z0-9.]+|"
    r"(?:[cmrtgipdhnxz][0-9a-z]?)\.(?:nano|micro|small|medium|large|xlarge|[2-9]xlarge)|"
    r"[a-z][0-9][a-z]?\.[a-z0-9.]+)$",
    re.I,
)

def _s(v) -> str:
    return v if isinstance(v, str) else ("" if v is None else str(v))

def _normalize(s: str) -> str:
    return (s or "").strip().lower()

def _last_token(s: str) -> str:
    s = _s(s)
    if not s:
        return ""
    t = re.split(r"[/:]", s)
    return t[-1] if t else s

def _is_region_like(s: str) -> bool:
    s = _s(s).strip()
    if not s:
        return False
    return bool(REGION_RE.match(s))

def _is_noise(s: str) -> bool:
    s = _s(s)
    if not s:
        return False
    # í‘œì¤€ íŒ¨í„´/ARN/ê²½ë¡œëŠ” ë…¸ì´ì¦ˆ ì•„ë‹˜
    if ARN_RE.match(s) or VOL_RE.search(s) or I_RE.search(s) or SG_RE.search(s) or ENI_RE.search(s) or SNAP_RE.search(s):
        return False
    if TG_PATH_RE.search(s) or ALB_PATH_RE.search(s) or NLB_PATH_RE.search(s):
        return False
    return bool(NOISED := NOISE_TOKEN_RE.match(s))

def _arn_resource_part(arn: str) -> str:
    m = ARN_RE.match(_s(arn))
    if not m:
        return _s(arn)
    return m.group(5) or ""

def _shorten_arn_generic(arn: str) -> str:
    arn = _s(arn)
    res = _arn_resource_part(arn)

    if IAM_ARN_RE.match(arn):
        return res.split("/", 1)[1] if "/" in res else res
    if LAMBDA_FN_ARN_RE.search(arn):
        return _last_token(res)
    if res.startswith("table/"):
        return res.split("/", 1)[1]
    if ":sns:" in arn or ":sqs:" in arn:
        return _last_token(res)
    if ":ecr:" in arn and res.startswith("repository/"):
        return res.split("/", 1)[1]
    if ":ecs:" in arn and (res.startswith("service/") or res.startswith("cluster/")):
        return res.split("/", 1)[1]
    if ":eks:" in arn and res.startswith("cluster/"):
        return res.split("/", 1)[1]
    if ":rds:" in arn and (res.startswith("db:") or res.startswith("cluster:")):
        return res.split(":", 1)[1]
    if ":kms:" in arn and (res.startswith("key/") or res.startswith("alias/")):
        return res.split("/", 1)[1]
    return _last_token(res)

def _shorten_arn(arn: str) -> str:
    return _shorten_arn_generic(arn)

def _dict_from_meta(meta_headers: List[str], meta_values: List[str]) -> Dict[str, str]:
    d = {}
    for h, v in zip(meta_headers or [], meta_values or []):
        d[_normalize(_s(h))] = _s(v).strip()
    return d

def _pick_first(rx: re.Pattern, texts: List[str]) -> str:
    for t in texts or []:
        ts = _s(t)
        m = rx.search(ts)
        if m:
            return m.group(0)
    return ""

def _has_lambda_version(texts: List[str]) -> str:
    for v in texts or []:
        sv = _s(v)
        if LAMBDA_FN_ARN_RE.search(sv) and (":$" in sv or sv.endswith(":$LATEST")):
            return sv
    return ""

def _is_human_name(s: str) -> bool:
    s = _s(s)
    if _is_noise(s) or _is_region_like(s):
        return False
    # â–¼ íƒ€ì… í† í°ì€ ì‚¬ëŒì¹œí™” ì•„ë‹˜ (ì˜ˆ: db.t3.micro, c6i.2xlarge ë“±)
    if INSTANCE_CLASS_RE.match(s):
        return False
    return bool(HUMAN_NAME_RE.match(s)) and not (VOL_RE.search(s) or I_RE.search(s))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ResourceId â†’ ì‚¬ëŒì¹œí™” (ì„œë¹„ìŠ¤ ê³µí†µ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def display_from_resource_id(resource_id: str, meta_vals: List[str]) -> str:
    rid = _s(resource_id).strip()
    if not rid or _is_noise(rid) or _is_region_like(rid):
        return ""

    for rx in [VOL_RE, I_RE, SG_RE, ENI_RE, SNAP_RE]:
        m = rx.search(rid)
        if m:
            return m.group(0)

    for rx in [TG_PATH_RE, ALB_PATH_RE, NLB_PATH_RE]:
        m = rx.search(rid)
        if m:
            segs = m.group(0).split("/")
            return segs[1] if len(segs) >= 2 else m.group(0)

    if LAMBDA_FN_ARN_RE.search(rid):
        if ":$" in rid or rid.endswith(":$LATEST"):
            return rid
        if ARN_RE.match(rid):
            return _shorten_arn(rid)
        return rid

    if ARN_RE.match(rid):
        full = _has_lambda_version(meta_vals)
        if full:
            return full
        return _shorten_arn(rid)

    if BUCKETISH_RE.match(rid) or _is_human_name(rid) or ("/" in rid and _is_human_name(rid)):
        return rid

    return ""

# --- ADD: Name ì „ìš© ì‚¬ëŒì¹œí™” íŒì • (ë‚œìˆ˜/ë¦¬ì „/ARN/íƒ€ì…í† í° ì œì™¸, ê·¸ ì™¸ ë„ë„íˆ í—ˆìš©)
def _is_human_friendly_for_name(s: str, *, header_key: str = "") -> bool:
    s = _s(s).strip()
    if not s:
        return False
    if s.lower().startswith("arn:"):
        return False
    if _is_region_like(s):
        return False
    if INSTANCE_CLASS_RE.match(s):  # db.t3.micro ë“±
        return False

    k = _normalize(_s(header_key))
    # Name/ì´ë¦„ í—¤ë”ì¼ ë•Œ: RDS ì‹ë³„ì í˜•íƒœëŠ” í—ˆìš© (NOISE_TOKEN_REì— ê±¸ë ¤ë„ í†µê³¼)
    if ("name" in k) or ("ì´ë¦„" in k):
        if RDS_IDENTIFIER_RE.match(s):
            return True

    # ì¼ë°˜ ê·œì¹™: ë„ˆë¬´ ë‚œìˆ˜ìŠ¤ëŸ¬ìš´ í† í°ì€ ì°¨ë‹¨
    if _is_noise(s):
        return False
    return True



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Strong ë©”íƒ€ í‚¤(ë¦¬ì†ŒìŠ¤ ì´ë¦„/IDê¸‰)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STRONG_META_KEYS_ORDER = [
    "bucket name",
    "role name", "user name", "group name", "policy name",
    "iam role", "iam user", "iam group", "iam policy",
    "function name", "lambda function", "function",
    "service name", "cluster name", "task definition", "workload",
    "load balancer name", "elb name", "alb name", "nlb name", "target group",
    "queue name", "topic name",
    "table name", "db identifier", "db id", "cluster id", "database name",
    "repository name",
    "distribution id",
    "domain name", "hosted zone name", "hosted zone id",
    "resource name", "name", "display name", "id",
    "instance id", "volume id", "volume arn", "security group id", "vpc id",
    "cache cluster id", "cache node type",
]

def pick_strong_meta_id(meta_map: Dict[str, str], meta_vals: List[str]) -> str:
    for key in STRONG_META_KEYS_ORDER:
        v = meta_map.get(key)
        if not v or _is_noise(v) or _is_region_like(v):
            continue
        if ARN_RE.match(v):
            if LAMBDA_FN_ARN_RE.search(v) and (":$" in v or v.endswith(":$LATEST")):
                return v
            return _shorten_arn(v)
        if any(rx.search(v) for rx in [VOL_RE, I_RE, SG_RE, ENI_RE, SNAP_RE]):
            return v
        for rx in [TG_PATH_RE, ALB_PATH_RE, NLB_PATH_RE]:
            m = rx.search(v)
            if m:
                segs = m.group(0).split("/")
                return segs[1] if len(segs) >= 2 else m.group(0)
        if CFN_DISTR_ID_RE.match(v):
            return v
        if BUCKETISH_RE.match(v) or _is_human_name(v):
            return v

    mvals = [_s(x) for x in (meta_vals or []) if _s(x) and not _is_noise(x) and not _is_region_like(x)]

    for rx in [IAM_ARN_RE, RDS_ARN_RE, KMS_ARN_RE, DDB_ARN_RE, SNS_ARN_RE, SQS_ARN_RE,
               ECR_ARN_RE, EKS_ARN_RE, ECS_ARN_RE, OPENSEARCH_ARN_RE, REDSHIFT_ARN_RE]:
        hit = _pick_first(rx, mvals)
        if hit:
            return _shorten_arn(hit)

    for rx in [TG_PATH_RE, ALB_PATH_RE, NLB_PATH_RE]:
        hit = _pick_first(rx, mvals)
        if hit:
            segs = hit.split("/")
            return segs[1] if len(segs) >= 2 else hit

    hit = _has_lambda_version(mvals)
    if hit:
        return hit
    for val in mvals:
        if LAMBDA_FN_ARN_RE.search(val):
            return _shorten_arn(val)

    hit = _pick_first(CFN_DISTR_ID_RE, mvals)
    if hit:
        return hit

    for val in mvals:
        if ROUTE53_ZONE_NAME_RE.match(val):
            return val

    for val in mvals:
        if BUCKETISH_RE.match(val) and "." not in val:
            return val

    for val in mvals:
        if _is_human_name(val):
            return val

    return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Name/Instance/Check ì „ìš©/ì¡°í•© ë¹Œë” (ì—…ë°ì´íŠ¸ í¬í•¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# --- REPLACE THIS FUNCTION ---
def pick_name_tab(meta_headers: List[str], meta_map: Dict[str, str]) -> str:
    """
    2ìˆœìœ„: meta_headers ìˆœì„œë¡œ ìŠ¤ìº”.
    í—¤ë”ì— 'name' ë˜ëŠ” 'ì´ë¦„'ì´ í¬í•¨ë˜ë©´, ê°’ì´ ì‚¬ëŒì¹œí™”(ì™„í™”)ì¼ ë•Œ ì¦‰ì‹œ ì±„íƒ.
    """
    for h in meta_headers or []:  # í—¤ë” ìˆœì„œ ë³´ì¡´
        k = _normalize(_s(h))
        if ("name" in k) or ("ì´ë¦„" in k):
            v = meta_map.get(k, "")
            if _is_human_friendly_for_name(v, header_key=k):
                return v
    return ""


# --- REPLACE THIS FUNCTION ---
# --- REPLACE THIS FUNCTION ---
# --- REPLACE THIS FUNCTION COMPLETELY ---
# --- REPLACE WHOLE FUNCTION ---
# --- REPLACE WHOLE FUNCTION ---
# --- REPLACE WHOLE FUNCTION ---
def pick_instance_tab(meta_headers: List[str], meta_map: Dict[str, str]) -> str:
    """
    4ìˆœìœ„: 'instance'ê°€ ë“¤ì–´ê°„ í—¤ë” ê°’ì„, í—¤ë”(meta_headers) ìˆœì„œ ìš°ì„ ìœ¼ë¡œ ì±„íƒ.
    - 'instance type' / 'instance class'ëŠ” ì œì™¸(ëŒ€í‘œ ë¦¬ì†ŒìŠ¤ê°€ ì•„ë‹˜)
    - ê°’ì´ ë¹„ì–´ìˆì§€ë§Œ ì•Šìœ¼ë©´ ê´€ëŒ€í•˜ê²Œ ì±„íƒ
      * ë¦¬ì „/AZ/ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… í† í°/ë‚œìˆ˜ í† í°ë§Œ ì œì™¸
      * ARNì€ 'ê·¸ëŒ€ë¡œ' ë°˜í™˜(ì¶•ì•½í•˜ì§€ ì•ŠìŒ)
    - í•œêµ­ì–´/ì¼ë³¸ì–´ í—¤ë”ë„ ì§€ì›
    """
    def _accept(v: str) -> str:
        v = (v or "").strip()
        if not v:
            return ""
        # íƒ€ì…í† í°/ë¦¬ì „/ë…¸ì´ì¦ˆ ì œì™¸
        if INSTANCE_CLASS_RE.match(v) or _is_region_like(v) or _is_noise(v):
            return ""
        # ARNì€ ê·¸ëŒ€ë¡œ í†µê³¼ (ì¶•ì•½ ê¸ˆì§€)
        if ARN_RE.match(v):
            return v
        return v

    # 1) í—¤ë” ìˆœì„œ ìš°ì„  ìŠ¤ìº” (ì˜ˆ: 'DB Instance'ê°€ ê°€ì¥ ë¨¼ì €ë©´ ê·¸ê±¸ ìš°ì„ )
    for h in meta_headers or []:
        k = _normalize(_s(h))
        if (("instance" in k) or ("ì¸ìŠ¤í„´ìŠ¤" in k) or ("ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹" in k)) and ("type" not in k) and ("class" not in k):
            acc = _accept(meta_map.get(k))
            if acc:
                return acc

    # 2) ê·¸ë˜ë„ ì—†ìœ¼ë©´ ëª¨ë“  í‚¤ ì¬ìŠ¤ìº”(ë¶€ë¶„ í¬í•¨ í—ˆìš©)
    for k, v in meta_map.items():
        kk = _normalize(k)
        if (("instance" in kk) or ("ì¸ìŠ¤í„´ìŠ¤" in kk) or ("ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹" in kk)) and ("type" not in kk) and ("class" not in kk):
            acc = _accept(v)
            if acc:
                return acc

    return ""








CHECK_TO_KEYS: List[Tuple[re.Pattern, List[str]]] = [
    (re.compile(r"\bnat\s+gateway\s+az\s+independence\b", re.I), ["nat id", "nat gateway id", "nat gateway"]),
]

def pick_check_specific(check_name: str, meta_map: Dict[str, str]) -> str:
    for pat, keys in CHECK_TO_KEYS:
        if pat.search(check_name or ""):
            for k in keys:
                v = meta_map.get(k)
                if v and not _is_noise(v) and not _is_region_like(v):
                    return v
    return ""

# â”€â”€ Service Limits / Quotas ë™ì˜ì–´ í…Œì´ë¸” â”€â”€
SERVICE_LIMITS_SYNONYMS: Dict[str, List[str]] = {
    "region": ["region", "aws region", "location"],
    "service": ["service", "service name"],
    "limit_amount": ["limit amount", "limit value", "limit", "limit name", "quota", "quota name", "quota value"],
    "current_usage": ["current usage", "in use", "usage", "current value", "used"],
    "status": ["status", "limit status", "violation status"],
}

def _get_by_synonyms_in_order(meta_headers: List[str], meta_map: Dict[str, str], keys: List[str]) -> str:
    """
    meta_headers ìˆœì„œ ìš°ì„ , ì •í™• ì¼ì¹˜ â†’ ë¶€ë¶„ í¬í•¨ ìˆœ.
    """
    lh = [_normalize(_s(h)) for h in (meta_headers or [])]
    # ì •í™• ì¼ì¹˜
    for k in keys:
        kk = _normalize(k)
        if kk in meta_map and meta_map[kk]:
            return meta_map[kk]
    # ë¶€ë¶„ í¬í•¨ (í—¤ë” ìˆœì„œ ë³´ì¡´)
    for h in lh:
        for k in keys:
            if _normalize(k) in h:
                v = meta_map.get(h)
                if v:
                    return v
    return ""

def pick_service_limits_combo(meta_headers: List[str], meta_map: Dict[str, str], check_name: str) -> str:
    n = _normalize(check_name)
    if not any(tok in n for tok in ["service limit", "service limits", "service quota", "service quotas", "quota", "limit"]):
        return ""

    region        = _get_by_synonyms_in_order(meta_headers, meta_map, SERVICE_LIMITS_SYNONYMS["region"])
    service       = _get_by_synonyms_in_order(meta_headers, meta_map, SERVICE_LIMITS_SYNONYMS["service"])
    limit_amount  = _get_by_synonyms_in_order(meta_headers, meta_map, SERVICE_LIMITS_SYNONYMS["limit_amount"])
    current_usage = _get_by_synonyms_in_order(meta_headers, meta_map, SERVICE_LIMITS_SYNONYMS["current_usage"])
    status        = _get_by_synonyms_in_order(meta_headers, meta_map, SERVICE_LIMITS_SYNONYMS["status"])

    parts = [p for p in [region, service, limit_amount, current_usage, status] if p and not _is_noise(p)]
    parts = [p for p in parts if not _is_region_like(p) or p != region]  # region í—ˆìš©í•˜ë˜ ë‹¨ë… ë…¸ì¶œ ë°©ì§€
    return " | ".join(parts) if parts else ""

def pick_recommendation_summary(meta_headers: List[str], meta_map: Dict[str, str]) -> str:
    """
    8ìˆœìœ„: ì¶”ì²œ ìš”ì•½ì„ ìµœí›„ ë³´ë£¨ë¡œ ì‚¬ìš©.
    """
    targets = ["recommended resource summary", "recommendation summary"]
    for h in meta_headers or []:
        k = _normalize(_s(h))
        if any(t == k for t in targets) or any(t in k for t in targets):
            v = meta_map.get(k, "")
            if v:
                return v
    return ""

def build_region_reason_combo(meta_map: Dict[str, str]) -> str:
    region = meta_map.get("region") or meta_map.get("aws region") or ""
    reason = meta_map.get("reason") or ""
    parts = [p for p in [region, reason] if p and not _is_noise(p)]
    if len(parts) >= 2:
        return " | ".join(parts)
    return ""

def build_full_combo(meta_headers: List[str], meta_map: Dict[str, str]) -> str:
    out: List[str] = []
    for h in meta_headers or []:
        key = _normalize(_s(h))
        v = meta_map.get(key)
        if not v:
            continue
        # í™˜ê²½ì„± ì»¬ëŸ¼/ê°’ ë°°ì œ
        if "region" in key or "az" in key:
            continue
        if _is_noise(v) or _is_region_like(v) or AZ_RE.match(v):
            continue
        out.append(v)
    return " | ".join(out) if out else ""



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„œë¹„ìŠ¤ë³„ ì¡°í•©/ìš”ì•½ (ë³´ì¡°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compose_combo_or_summary(check_name: str, meta_map: Dict[str, str], meta_vals: List[str]) -> str:
    n = _normalize(check_name)

    if ("service limit" in n) or ("service quota" in n):
        fields = ["region", "service", "limit amount", "current usage", "status"]
        parts = [meta_map.get(k, "") for k in fields if meta_map.get(k)]
        parts = [p for p in parts if p and not _is_noise(p)]
        if parts and len(parts) >= 2:
            return " | ".join(parts)

    if "unassociated elastic ip" in n or ("elastic ip" in n and "unassociated" in n):
        region = meta_map.get("region") or meta_map.get("aws region") or ""
        ip = meta_map.get("ip address") or meta_map.get("public ip address") or meta_map.get("public ip") or ""
        alloc = meta_map.get("allocation id") or meta_map.get("allocation-id") or ""
        parts = [x for x in [region, ip, alloc] if x and not _is_noise(x)]
        if len(parts) >= 2:
            return " | ".join(parts)

    if ("savings plan" in n) or ("savings plans" in n) or ("purchase recommendation" in n) or ("reserved node" in n) or ("recommendation" in n):
        for k, v in meta_map.items():
            if ("recommended resource summary" in k) or ("recommendation summary" in k):
                if v and not _is_noise(v):
                    return v

    return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”íƒ€ì˜ 'Resource Id'/'ResourceId'/'Resource'ì—ì„œ ResourceId ëŒ€ì²´ (1ìˆœìœ„ ê·¸ë£¹ ë‚´ë¶€, ë³´ê°•)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _from_meta_resource_like(meta_map: Dict[str, str], meta_vals: List[str]) -> str:
    # ìš°ì„ ìˆœìœ„: resource id â†’ resourceid â†’ resource â†’ identifier
    for key in ("resource id", "resourceid", "resource", "identifier"):
        v = meta_map.get(key)
        if not v:
            continue

        if key in ("resource id", "resourceid"):
            disp = display_from_resource_id(v, meta_vals)
            if disp:
                return disp
            continue

        if key == "resource":
            # 'resource'ê°€ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…/ë¦¬ì „/ë…¸ì´ì¦ˆë©´ íŒ¨ìŠ¤í•´ì„œ ë‹¤ìŒ ë‹¨ê³„ë¡œ
            if INSTANCE_CLASS_RE.match(v) or _is_region_like(v) or _is_noise(v):
                continue
            if ARN_RE.match(v) or any(rx.search(v) for rx in [VOL_RE, I_RE, SG_RE, ENI_RE, SNAP_RE, TG_PATH_RE, ALB_PATH_RE, NLB_PATH_RE]):
                disp = display_from_resource_id(v, meta_vals)
                if disp:
                    return disp
                return v
            if _is_human_name(v):
                return v
            continue

        if key == "identifier":
            # identifier ê°’ë„ ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬
            if INSTANCE_CLASS_RE.match(v) or _is_region_like(v) or _is_noise(v):
                continue
            if ARN_RE.match(v) or any(rx.search(v) for rx in [VOL_RE, I_RE, SG_RE, ENI_RE, SNAP_RE, TG_PATH_RE, ALB_PATH_RE, NLB_PATH_RE]):
                disp = display_from_resource_id(v, meta_vals)
                if disp:
                    return disp
                return v
            if _is_human_name(v):
                return v

    return ""


# --- ADD: 1ìˆœìœ„ìš© (ë©”íƒ€) Identifier ì¶”ì¶œê¸° (í—¤ë” ìˆœì„œ ì¡´ì¤‘)
def _from_meta_identifier(meta_headers: List[str], meta_map: Dict[str, str], meta_vals: List[str]) -> str:
    """
    ë©”íƒ€ í—¤ë” ì¤‘ 'identifier'ê°€ ë“¤ì–´ê°„ ì—´ì„ í—¤ë” ìˆœì„œëŒ€ë¡œ ìŠ¤ìº”í•˜ì—¬ ëŒ€í‘œê°’ ë°˜í™˜.
    - 'type'ì´ ë¶™ì€ í—¤ë”(ì˜ˆ: 'instance type identifier')ëŠ” ì œì™¸
    - ê°’ì´ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… í† í°(ì˜ˆ: db.t3.micro)ì¸ ê²½ìš° ì œì™¸
    - ARN/ID/ê²½ë¡œë©´ ì¶•ì•½ ë˜ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
    - ê·¸ ì™¸ì—ëŠ” ì‚¬ëŒì¹œí™” ê°’ë§Œ í—ˆìš©
    """
    for h in meta_headers or []:
        k = _normalize(_s(h))
        if "identifier" in k and "type" not in k:  # 'type' í¬í•¨ ì—´ì€ ì œì™¸
            v = (meta_map.get(k) or "").strip()
            if not v:
                continue
            # ê°’ì´ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì´ë©´ ì œì™¸
            if INSTANCE_CLASS_RE.match(v):
                continue
            # ë¦¬ì „/ë…¸ì´ì¦ˆ ì œì™¸
            if _is_region_like(v) or _is_noise(v):
                continue
            # ARN/ID/ê²½ë¡œ í˜•íƒœë©´ ì¶•ì•½ ì²˜ë¦¬
            if ARN_RE.match(v) or any(rx.search(v) for rx in [VOL_RE, I_RE, SG_RE, ENI_RE, SNAP_RE, TG_PATH_RE, ALB_PATH_RE, NLB_PATH_RE]):
                disp = display_from_resource_id(v, meta_vals)
                if disp:
                    return disp
                return v
            # ì‚¬ëŒì¹œí™” ê°’ì´ë©´ í—ˆìš©
            if _is_human_name(v):
                return v
    return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìµœì¢… í‘œì‹œ ë¬¸ìì—´ ìƒì„± â€” â˜…ìš°ì„ ìˆœìœ„ ì²´ì¸ ë°˜ì˜â˜…
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compose_display(check_name: str, meta_headers: List[str], meta_values: List[str], resource_id: str) -> str:
    """
    ìš°ì„ ìˆœìœ„:
    1) ResourceId / (ë©”íƒ€) Resource Id / Resource(ë³´ê°•)
    2) Name í¬í•¨ íƒ­ (meta_headers ìˆœì„œ)
    3) Strong
    4) Instance í¬í•¨ íƒ­/ì¡°í•©
    5) ì²´í¬ ì „ìš© í‚¤
    6) Service Limits/Quotas ì¡°ë¦½ (Region | Service | Limit Amount | Current Usage | Status)
    7) ì „ì²´ ì¡°í•©
    8) Recommended Resource Summary / Recommendation Summary (ìµœí›„ ë³´ë£¨)
    """
    meta_vals = [_s(v) for v in (meta_values or []) if _s(v)]
    meta_map = _dict_from_meta(meta_headers, meta_vals)

# --- REPLACE THE PRIORITY BLOCK INSIDE compose_display() ---
    # 1) ResourceId / (ë©”íƒ€) Resource Id / (ë©”íƒ€) Resource / (ë©”íƒ€) Identifier
    v1 = display_from_resource_id(resource_id, meta_vals)
    if v1:
        return v1
    v1b = _from_meta_resource_like(meta_map, meta_vals)  # resource id / resourceid / resource / identifier (ì •í™•í‚¤)
    if v1b:
        return v1b
    v1c = _from_meta_identifier(meta_headers, meta_map, meta_vals)  # í—¤ë” ìˆœì„œ ê¸°ì¤€ '...identifier...' ì—´
    if v1c:
        return v1c

    # 2) í—¤ë”ì— Name í¬í•¨ (ì˜ˆ: DB Instance Name, Load Balancer Name, â€¦)
    v2 = pick_name_tab(meta_headers, meta_map)
    if v2:
        return v2

    # 4) Strong
    v4 = pick_strong_meta_id(meta_map, meta_vals)
    if v4:
        return v4

    # 5) Instanceê°€ ë“¤ì–´ê°„ íƒ­/ì¡°í•©
    v5 = pick_instance_tab(meta_headers, meta_map)
    if v5:
        return v5

    # 6) Check ì „ìš© í‚¤
    v6 = pick_check_specific(check_name, meta_map)
    if v6:
        return v6

    # 7) íŠ¹ì • ì¡°í•© (Region | Reason)
    v7 = build_region_reason_combo(meta_map)
    if v7:
        return v7

    # ğŸ‘‰ ì•ˆì „ì¥ì¹˜: 1~6ë‹¨ê³„ê°€ ë¹„ì–´ì„œ 7ë²ˆ(ì „ì²´ ì¡°í•©)ìœ¼ë¡œ ê°€ê¸° ì§ì „,
    #    'instance' í—¤ë” ê°’ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì±„íƒ (ìš°ì„ ìˆœìœ„ ë³´ì¡´)
    inst_guard = pick_instance_tab(meta_headers, meta_map)
    if inst_guard:
        return inst_guard


    # 8) ì „ì²´ ì¡°í•©
    v8 = build_full_combo(meta_headers, meta_map)
    if v8:
        return v8

    return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TA ê²°ê³¼ ì¡°íšŒ : ìƒíƒœ ì¹´ìš´íŠ¸ + (ì£¼ì˜/ê²½ê³ )ë³„ í‘œì‹œ ë¬¸ìì—´ ëª©ë¡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_check_result_summary(
    support, check_id: str, meta_headers: List[str], check_name: str
) -> Tuple[int, int, int, int, str, Dict[str, List[str]]]:
    warn_count = 0
    error_count = 0
    ok_count = 0
    excluded_count = 0

    next_token = None
    result_status_raw = None
    status_to_texts: DefaultDict[str, List[str]] = defaultdict(list)

    while True:
        for attempt in range(6):
            try:
                kwargs = {"checkId": check_id, "language": LANGUAGE}
                if next_token:
                    kwargs["nextToken"] = next_token
                resp = support.describe_trusted_advisor_check_result(**kwargs)
                break
            except botocore.exceptions.ClientError as e:
                code = e.response.get("Error", {}).get("Code", "")
                if code in {"ThrottlingException", "TooManyRequestsException"}:
                    backoff_sleep(attempt)
                    continue
                raise

        result = resp.get("result", {}) or {}

        if result_status_raw is None:
            result_status_raw = (result.get("status") or "").lower().strip() or "not_available"

        flagged = result.get("flaggedResources") or []
        for r in flagged:
            st_raw = (r.get("status") or "").lower().strip()
            rid = _s(r.get("resourceId")).strip()
            is_suppressed = bool(r.get("isSuppressed"))
            meta_vals = [ _s(v) for v in (r.get("metadata") or []) ]

            _debug_print_meta_for_rds_backups(check_name, meta_headers, meta_vals, rid, st_raw)

            display_text = compose_display(check_name, meta_headers, meta_vals, rid)

            if is_suppressed:
                excluded_count += 1

            if st_raw == "error":
                error_count += 1
            elif st_raw == "warning":
                warn_count += 1
            elif st_raw == "ok":
                ok_count += 1

            if (not is_suppressed) and display_text:
                if st_raw in ("ok", "warning", "error"):
                    status_to_texts[st_raw].append(display_text)

        next_token = resp.get("nextToken")
        if not next_token:
            break

    overall = STATUS_MAP_FINAL.get(result_status_raw, "ë¯¸í™•ì¸")
    return error_count, warn_count, ok_count, excluded_count, overall, status_to_texts

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¬¸ìì—´ ìœ í‹¸ ë° "ë¦¬ì†ŒìŠ¤ ê°’" ë¬¸ìì—´ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def split_on_commas(s: str) -> List[str]:
    s = _s(s)
    if not s:
        return []
    s = s.replace("ï¼Œ", ",").replace("\u00A0", " ")
    s = s.replace("\r\n", " ").replace("\r", " ").replace("\n", " ")
    parts = re.split(r",\s*", s)
    return [p.strip() for p in parts if p and p.strip()]

def explode_unique(items: List[str]) -> List[str]:
    seen = set()
    out: List[str] = []
    for s in items or []:
        toks = split_on_commas(s) if ("," in _s(s)) else [_s(s)]
        for tok in toks:
            tok = tok.strip()
            if tok and tok not in seen:
                seen.add(tok)
                out.append(tok)
    return out

def build_resource_value_cell(status_to_texts: Dict[str, List[str]]) -> str:
    lines: List[str] = []
    order = [("warning", "ì£¼ì˜"), ("error", "ê²½ê³ ")]  # ì •ìƒ(ok) ì œì™¸
    for key, label in order:
        raw = status_to_texts.get(key, []) or []
        texts = explode_unique(raw)
        if not texts:
            continue
        lines.append(f"{label} :")
        lines.extend(texts)
    return "\n".join(lines)

# â”€â”€ ì†ŒìŠ¤ ì¢…ë¥˜ íŒë³„ â”€â”€
SOURCE_RULES = [
    (re.compile(r"cost\s*optimization\s*hub", re.I), "AWS Cost Optimization Hub"),
    (re.compile(r"(?:compute|computer)\s*optimizer", re.I), "AWS Compute Optimizer"),
    (re.compile(r"\bconfig\b|aws\s*config|config\s*rule", re.I), "AWS Config"),
    (re.compile(r"well[-\s]?architected", re.I), "Well Architected ë¦¬ë·°"),
    (re.compile(r"security\s*hub", re.I), "AWS Security hub"),
    (re.compile(r"\brds\b|amazon\s*rds", re.I), "Amazon RDS"),
]

def detect_source(name: str, description: str, category: str) -> str:
    blob = " ".join([(name or ""), (description or ""), (category or "")])
    for pat, label in SOURCE_RULES:
        if pat.search(blob):
            return label
    return "AWS Trusted Advisor"

def main():
    print(f"[INFO] Running file : {__file__}")
    print(f"[INFO] Script ver  : {VERSION}")

    support = session.client("support", region_name=SUPPORT_REGION)

    # ì²´í¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
    try:
        resp = support.describe_trusted_advisor_checks(language=LANGUAGE)
        checks: List[dict] = resp.get("checks", []) or []
        print(f"[DEBUG] checks(en): {len(checks)}")
        if not checks and LANGUAGE.lower() != "ja":
            print("[WARN] No checks with LANGUAGE='en'. Retrying with LANGUAGE='ja' ...")
            resp = support.describe_trusted_advisor_checks(language="ja")
            checks = resp.get("checks", []) or []
            print(f"[DEBUG] checks(ja): {len(checks)}")
    except botocore.exceptions.ClientError as e:
        code = e.response.get("Error", {}).get("Code", "")
        if code in {"SubscriptionRequiredException", "AccessDeniedException"}:
            sys.stderr.write(
                "[ì—ëŸ¬] Trusted Advisor API ì‚¬ìš© ê¶Œí•œ/ì§€ì› í”Œëœ ë¶€ì¡±.\n"
                " - Business/Enterprise/On-Ramp ì§€ì› í”Œëœ í•„ìš”\n"
                " - ë˜ëŠ” aws-support ê´€ë ¨ ê¶Œí•œ í•„ìš”\n"
            )
        elif code == "UnhandledValidationException":
            sys.stderr.write("[ì—ëŸ¬] language ê°’ì€ 'en' ë˜ëŠ” 'ja'ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.\n")
        else:
            sys.stderr.write(f"[ì—ëŸ¬] API í˜¸ì¶œ ì‹¤íŒ¨: {code}: {e}\n")
        sys.exit(1)
    except botocore.exceptions.NoCredentialsError:
        sys.stderr.write("[ì—ëŸ¬] ìê²© ì¦ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")
        sys.exit(1)

    # ë¹„ì–´ ìˆì–´ë„ "í—¤ë”ë§Œ ìˆëŠ” ì—‘ì…€" ìƒì„±
    if not checks:
        print("ê°€ì ¸ì˜¬ Trusted Advisor ì²´í¬ê°€ ì—†ìŠµë‹ˆë‹¤. (í—¤ë”ë§Œ ìƒì„±)")
        df_all = pd.DataFrame(columns=[
            "Check ID","Name","Category","Description",
            "ê²½ê³ (ì¡°ì¹˜ ê¶Œê³ )","ì£¼ì˜(ì¡°ì‚¬ ê¶Œê³ )","ì •ìƒ(ë¬¸ì œ ì—†ìŒ)","ì œì™¸",
            "ìµœì¢… ìƒíƒœ","ë¦¬ì†ŒìŠ¤ ê°’","ì†ŒìŠ¤ ì¢…ë¥˜"
        ])
        with pd.ExcelWriter(OUTPUT_XLSX, engine="openpyxl") as xw:
            df_all.to_excel(xw, index=False, sheet_name="All")
            ws = xw.sheets["All"]
            for col_name, width in [("ë¦¬ì†ŒìŠ¤ ê°’", 80), ("ìµœì¢… ìƒíƒœ", 14), ("ì†ŒìŠ¤ ì¢…ë¥˜", 28)]:
                if col_name in df_all.columns:
                    idx = df_all.columns.get_loc(col_name) + 1
                    ws.column_dimensions[get_column_letter(idx)].width = width
        print(f"ì™„ë£Œ: {OUTPUT_XLSX} (ë¹ˆ ê²°ê³¼, í—¤ë”ë§Œ)")
        return

    rows = []
    sample_preview = None

    for c in checks:
        check_id = c.get("id")
        check_name = c.get("name") or ""
        category = pretty_pillar_name(c.get("category"))
        description = (c.get("description") or "").strip()
        meta_headers = c.get("metadata") or []   # ë©”íƒ€ë°ì´í„° í—¤ë”

        try:
            error_cnt, warn_cnt, ok_cnt, excl_cnt, overall, status_to_texts = (
                fetch_check_result_summary(support, check_id, meta_headers, check_name)
            )
        except botocore.exceptions.ClientError as e:
            sys.stderr.write(f"[ê²½ê³ ] ì²´í¬ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨ ({check_id}, {check_name}): {e}\n")
            error_cnt = warn_cnt = ok_cnt = excl_cnt = 0
            overall = "ë¯¸í™•ì¸"
            status_to_texts = {}

        resource_value = build_resource_value_cell(status_to_texts)
        source_type = detect_source(name=check_name, description=description, category=category)

        if not sample_preview and resource_value:
            sample_preview = resource_value.replace("\n", "â")

        rows.append(
            {
                "Check ID": check_id,
                "Name": check_name,
                "Category": category,
                "Description": description,
                "ê²½ê³ (ì¡°ì¹˜ ê¶Œê³ )": error_cnt,
                "ì£¼ì˜(ì¡°ì‚¬ ê¶Œê³ )": warn_cnt,
                "ì •ìƒ(ë¬¸ì œ ì—†ìŒ)": ok_cnt,
                "ì œì™¸": excl_cnt,
                "ìµœì¢… ìƒíƒœ": overall,
                "ë¦¬ì†ŒìŠ¤ ê°’": resource_value,
                "ì†ŒìŠ¤ ì¢…ë¥˜": source_type,
            }
        )

    df_all = pd.DataFrame(rows).sort_values(["Category", "Name"], ignore_index=True)

    with pd.ExcelWriter(OUTPUT_XLSX, engine="openpyxl") as xw:
        df_all.to_excel(xw, index=False, sheet_name="All")
        ws = xw.sheets["All"]

        if "ë¦¬ì†ŒìŠ¤ ê°’" in df_all.columns:
            col_idx = df_all.columns.get_loc("ë¦¬ì†ŒìŠ¤ ê°’") + 1
            ws.column_dimensions[get_column_letter(col_idx)].width = 80
            for row in range(2, ws.max_row + 1):
                cell = ws.cell(row=row, column=col_idx)
                cell.alignment = Alignment(wrap_text=True, vertical="top")

        if "ìµœì¢… ìƒíƒœ" in df_all.columns:
            col_idx = df_all.columns.get_loc("ìµœì¢… ìƒíƒœ") + 1
            ws.column_dimensions[get_column_letter(col_idx)].width = 14

        if "ì†ŒìŠ¤ ì¢…ë¥˜" in df_all.columns:
            col_idx = df_all.columns.get_loc("ì†ŒìŠ¤ ì¢…ë¥˜") + 1
            ws.column_dimensions[get_column_letter(col_idx)].width = 28

    print(f"ì™„ë£Œ: {OUTPUT_XLSX} ìƒì„± (All ì‹œíŠ¸)")
    if sample_preview:
        print(f"[PREVIEW] ì²« ë²ˆì§¸ 'ë¦¬ì†ŒìŠ¤ ê°’' (ê°œí–‰í‘œì‹œ=â): {sample_preview}")
    print(" - ìƒˆ ìš°ì„ ìˆœìœ„ ì ìš©: ResourceId/Resource(ë³´ê°•) â†’ Name(í—¤ë”ìˆœì„œ) â†’ Strong â†’ Instance â†’ Checkì „ìš© â†’ ServiceLimitsì¡°í•© â†’ ì „ì²´ì¡°í•© â†’ ì¶”ì²œìš”ì•½")
    print(" - ë‚œìˆ˜í˜• ë©”íƒ€í† í°/ë¦¬ì „ ë‹¨ë…/íƒ€ì…í† í°ì€ í•„í„°ë§")
    print(f"[INFO] Script version: {VERSION}")

if __name__ == "__main__":
    main()
